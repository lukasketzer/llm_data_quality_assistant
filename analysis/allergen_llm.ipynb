{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0da71cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the path so the package is importable\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from llm_data_quality_assistant import pipeline\n",
    "from llm_data_quality_assistant.corruptor import RowCorruptionTypes, CellCorruptionTypes\n",
    "from llm_data_quality_assistant.enums import Models\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20e2ac00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code ->\n",
      "    nuts,\n",
      "    almondnuts,\n",
      "    brazil_nuts,\n",
      "    macadamia_nuts,\n",
      "    hazelnut,\n",
      "    pistachio,\n",
      "    walnut,\n",
      "    cashew,\n",
      "    celery,\n",
      "    crustaceans,\n",
      "    eggs,\n",
      "    fish,\n",
      "    gluten,\n",
      "    lupin,\n",
      "    milk,\n",
      "    molluscs,\n",
      "    mustard,\n",
      "    peanut,\n",
      "    sesame,\n",
      "    soy,\n",
      "    sulfite\n",
      "\n",
      "-- Attribute contracts\n",
      "@nuts:integer\n",
      "@almondnuts:integer\n",
      "@brazil_nuts:integer\n",
      "@macadamia_nuts:integer\n",
      "@hazelnut:integer\n",
      "@pistachio:integer\n",
      "@walnut:integer\n",
      "@cashew:integer\n",
      "@celery:integer\n",
      "@crustaceans:integer\n",
      "@eggs:integer\n",
      "@fish:integer\n",
      "@gluten:integer\n",
      "@lupin:integer\n",
      "@milk:integer\n",
      "@molluscs:integer\n",
      "@mustard:integer\n",
      "@peanut:integer\n",
      "@sesame:integer\n",
      "@soy:integer\n",
      "@sulfite:integer\n",
      "\n",
      "\n",
      "The attributes\n",
      "of this dataset indicate the presence (‘2’), traces (‘1’), or absence\n",
      "(‘0’) of allergens in a product.\n",
      "\n",
      "Everything value except of the code has to be between 0 and 2.\n",
      "\n",
      "--NOT ALLOWED THAT\n",
      "nuts < almondnuts\n",
      "nuts < brazil_nuts\n",
      "nuts < macadamia_nuts\n",
      "nuts < hazelnut\n",
      "nuts < pistachio\n",
      "nuts < walnut\n",
      "nuts < cashew\n",
      "\n",
      "IT ALLWAYS MUST BE TRUE THAT:\n",
      "nuts >= almondnuts\n",
      "nuts >= brazil_nuts\n",
      "nuts >= macadamia_nuts\n",
      "nuts >= hazelnut\n",
      "nuts >= pistachio\n",
      "nuts >= walnut\n",
      "nuts >= cashew\n",
      "            code  nuts  almondnuts  brazil_nuts  macadamia_nuts  hazelnut  \\\n",
      "0  4104420006065     0           0            0               0         0   \n",
      "1  4104420006065     0           0            0               0         0   \n",
      "\n",
      "   pistachio  walnut  cashew  celery  ...  fish  gluten  lupin  milk  \\\n",
      "0          0       0       0       0  ...     0       2      0     0   \n",
      "1          0       0       0       0  ...     0       2      0     0   \n",
      "\n",
      "   molluscs  mustard  peanut  sesame  soy  sulfite  \n",
      "0         0        0       0       0    1        0  \n",
      "1         0        0       0       0    1        0  \n",
      "\n",
      "[2 rows x 22 columns]\n",
      "            code  nuts  almondnuts  brazil_nuts  macadamia_nuts  hazelnut  \\\n",
      "0  4104420006065     0           0            0               0         0   \n",
      "1  4104420006065     0           0            0               0         0   \n",
      "\n",
      "   pistachio  walnut  cashew  celery  ...  fish  gluten  lupin  milk  \\\n",
      "0          0       0       0       0  ...     0       2      0     0   \n",
      "1          0       0       0       0  ...     0       2      0     0   \n",
      "\n",
      "   molluscs  mustard  peanut  sesame  soy  sulfite  \n",
      "0         0        0       0       0    1        0  \n",
      "1         0        0       0       0    1        0  \n",
      "\n",
      "[2 rows x 22 columns]\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "corrupt_dataset = pd.read_csv(\"../datasets/parker_datasets/allergen/allergen_corrupted_first1000.csv\")\n",
    "\n",
    "gold_standard = pd.read_csv(\"../datasets/parker_datasets/allergen/allergen_cleaned_gold_first1000.csv\")\n",
    "\n",
    "with open(\"../datasets/parker_datasets/allergen/allergen.partialkey\", \"r\") as f:\n",
    "    partial_keys = f.read()\n",
    "\n",
    "with open(\"../datasets/parker_datasets/allergen/allergen.rules\", \"r\") as f:\n",
    "    rules = f.read()\n",
    "\n",
    "\n",
    "print(partial_keys)\n",
    "print(rules)\n",
    "print(corrupt_dataset.head(2))\n",
    "print(gold_standard.head(2))\n",
    "print(type(gold_standard.get(\"code\").iloc[0]))\n",
    "print(type(corrupt_dataset.get(\"code\").iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10cb1e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortened_corrupt_df = corrupt_dataset[corrupt_dataset[\"code\"].isin(gold_standard[\"code\"])]\n",
    "# shortened_corrupt_df = shortened_corrupt_df.sort_values(by=\"code\").reset_index(drop=True)\n",
    "\n",
    "# print(\"Shape shortened corrupt dataset:\")\n",
    "# print(shortened_corrupt_df.shape)\n",
    "\n",
    "# print(shortened_corrupt_df)\n",
    "\n",
    "# shortened_gold_standard = gold_standard[gold_standard[\"code\"].isin(shortened_corrupt_df[\"code\"])]\n",
    "# shortened_gold_standard = shortened_gold_standard.sort_values(by=\"code\").reset_index(drop=True)\n",
    "\n",
    "# print(\"Shape shortened gold standard dataset:\")\n",
    "# print(shortened_gold_standard.shape)\n",
    "\n",
    "# print(shortened_gold_standard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845980fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging groups with LLM:  93%|█████████▎| 96/103 [03:17<00:14,  2.06s/it]\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-lite'}, 'quotaValue': '1500'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '40s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m rpm = \u001b[32m30\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Merge/clean with LLM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m merged_df = \u001b[43mPipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge_with_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorrupt_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprimary_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprimary_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrpm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrpm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatus_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# # Show DataFrames for inspection\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# print(\"Merged DataFrame:\")\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# print(merged_df)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# print(\"Corrupted DataFrame:\")\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# print(shortened_corrupt_df)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/llm_data_quality_assistant/pipeline.py:88\u001b[39m, in \u001b[36mPipeline.merge_with_llm\u001b[39m\u001b[34m(dataset, primary_key, model_name, rpm, additional_prompt, verbose, status_bar)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmerge_with_llm\u001b[39m(\n\u001b[32m     78\u001b[39m     dataset: pd.DataFrame,\n\u001b[32m   (...)\u001b[39m\u001b[32m     86\u001b[39m     status_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     87\u001b[39m ) -> pd.DataFrame:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge_datasets_by_primary_key\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprimary_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprimary_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrpm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrpm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatus_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatus_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/llm_data_quality_assistant/llm_integration.py:96\u001b[39m, in \u001b[36mmerge_datasets_by_primary_key\u001b[39m\u001b[34m(model_name, primary_key, dataset, verbose, status_bar, rpm, additional_prompt)\u001b[39m\n\u001b[32m     93\u001b[39m row_order = group[\u001b[33m\"\u001b[39m\u001b[33m_tmp_row_order\u001b[39m\u001b[33m\"\u001b[39m].values\n\u001b[32m     94\u001b[39m group.drop(columns=[\u001b[33m\"\u001b[39m\u001b[33m_tmp_row_order\u001b[39m\u001b[33m\"\u001b[39m], inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m merged_row = \u001b[43m__merge_single_corrupted_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m merged_row[\u001b[33m\"\u001b[39m\u001b[33m_tmp_row_order\u001b[39m\u001b[33m\"\u001b[39m] = row_order\n\u001b[32m    103\u001b[39m merged_rows.append(merged_row)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/llm_data_quality_assistant/llm_integration.py:167\u001b[39m, in \u001b[36m__merge_single_corrupted_dataset\u001b[39m\u001b[34m(model_name, dataset, additional_prompt, verbose)\u001b[39m\n\u001b[32m    143\u001b[39m csv = dataset.to_csv(index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    145\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[33mYou are a data cleaning assistant. \u001b[39m\n\u001b[32m    147\u001b[39m \u001b[33mYou will be given a dataset about the same topic, but it may contain errors or inconsistencies. \u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    164\u001b[39m \u001b[33m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m merged_df = \u001b[43m__merge_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m merged_df.shape[\u001b[32m0\u001b[39m] == \u001b[32m0\u001b[39m:\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    175\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe merged DataFrame is empty. This might be due to the LLM not returning any data. Please check the model and prompt.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    176\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/llm_data_quality_assistant/llm_integration.py:46\u001b[39m, in \u001b[36m__merge_datasets\u001b[39m\u001b[34m(model_name, prompt, dataset, verbose)\u001b[39m\n\u001b[32m     43\u001b[39m         message += chunk\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     message = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mstruct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m data = json.loads(message)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/llm_data_quality_assistant/llm_models.py:156\u001b[39m, in \u001b[36mGeminiModel.generate\u001b[39m\u001b[34m(self, prompt, format)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     config.update(\n\u001b[32m    150\u001b[39m         {\n\u001b[32m    151\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_mime_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    152\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_schema\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mformat\u001b[39m,\n\u001b[32m    153\u001b[39m         }\n\u001b[32m    154\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    160\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.text \u001b[38;5;28;01mif\u001b[39;00m response.text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/venv/lib/python3.13/site-packages/google/genai/models.py:5630\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5628\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5629\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5630\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5631\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5632\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5633\u001b[39m   logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is done.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   5634\u001b[39m   remaining_remote_calls_afc -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/venv/lib/python3.13/site-packages/google/genai/models.py:4593\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4590\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   4591\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4593\u001b[39m response_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4594\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4595\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n\u001b[32m   4598\u001b[39m   response_dict = _GenerateContentResponse_from_vertex(\n\u001b[32m   4599\u001b[39m       \u001b[38;5;28mself\u001b[39m._api_client, response_dict\n\u001b[32m   4600\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/venv/lib/python3.13/site-packages/google/genai/_api_client.py:755\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    746\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    747\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    750\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    751\u001b[39m ) -> Union[BaseResponse, Any]:\n\u001b[32m    752\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m    753\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m    754\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m755\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    756\u001b[39m   json_response = response.json\n\u001b[32m    757\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/venv/lib/python3.13/site-packages/google/genai/_api_client.py:684\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    677\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m    678\u001b[39m       method=http_request.method,\n\u001b[32m    679\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    682\u001b[39m       timeout=http_request.timeout,\n\u001b[32m    683\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    685\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    686\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    687\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/venv/lib/python3.13/site-packages/google/genai/errors.py:101\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m     99\u001b[39m status_code = response.status_code\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    103\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-lite'}, 'quotaValue': '1500'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '40s'}]}}"
     ]
    }
   ],
   "source": [
    "# Clean and evaluate using the new Pipeline API\n",
    "from llm_data_quality_assistant.pipeline import Pipeline\n",
    "from llm_data_quality_assistant.enums import Models\n",
    "\n",
    "# Use a primary key for merging\n",
    "primary_key = \"code\"\n",
    "model = Models.GeminiModels.GEMINI_2_0_FLASH_LITE\n",
    "rpm = 30\n",
    "\n",
    "# Merge/clean with LLM\n",
    "merged_df = Pipeline.merge_with_llm(\n",
    "    dataset=corrupt_dataset,\n",
    "    primary_key=primary_key,\n",
    "    model_name=model,\n",
    "    rpm=rpm,\n",
    "    additional_prompt=rules,\n",
    "    verbose=False,\n",
    "    status_bar=True,\n",
    ")\n",
    "# # Show DataFrames for inspection\n",
    "# print(\"Merged DataFrame:\")\n",
    "# print(merged_df)\n",
    "# print(\"Gold Standard DataFrame:\")\n",
    "# print(shortened_gold_standard)\n",
    "# print(\"Corrupted DataFrame:\")\n",
    "# print(shortened_corrupt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4bac6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Evaluate results\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m stats_micro = \u001b[43mPipeline\u001b[49m.evaluate_micro(\n\u001b[32m      3\u001b[39m     gold_standard=gold_standard,\n\u001b[32m      4\u001b[39m     cleaned_dataset=merged_df,\n\u001b[32m      5\u001b[39m     corrupted_dataset=corrupt_dataset\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m====================================\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMICRO EVALUATION RESULTS\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate results\n",
    "stats_micro = Pipeline.evaluate_micro(\n",
    "    gold_standard=gold_standard,\n",
    "    cleaned_dataset=merged_df,\n",
    "    corrupted_dataset=corrupt_dataset\n",
    ")\n",
    "print(\"====================================\")\n",
    "print(\"MICRO EVALUATION RESULTS\")\n",
    "print(\"====================================\")\n",
    "pprint(stats_micro)\n",
    "\n",
    "stats_macro = Pipeline.evaluate_macro(\n",
    "    gold_standard=gold_standard,\n",
    "    cleaned_dataset=merged_df,\n",
    "    corrupted_dataset=corrupt_dataset\n",
    ")\n",
    "print(\"====================================\")\n",
    "print(\"MACRO EVALUATION RESULTS\")\n",
    "print(\"====================================\")\n",
    "pprint(stats_macro)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ead7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
