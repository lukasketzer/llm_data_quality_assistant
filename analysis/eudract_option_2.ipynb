{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed2839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the path so the package is importable\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from llm_data_quality_assistant.enums import Models\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import jupyter_helper_functions\n",
    "import string\n",
    "import time\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7781d3cd",
   "metadata": {},
   "source": [
    "# 2. Load and Explore EudraCT Data\n",
    "Load the EudraCT dataset and perform exploratory data analysis to understand its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08101e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eudract_number ->\n",
      "    single_blind,\n",
      "    double_blind,\n",
      "    open,\n",
      "    controlled,\n",
      "    placebo,\n",
      "    active_comparator,\n",
      "    randomised,\n",
      "    crossover,\n",
      "    parallel_group,\n",
      "    arms\n",
      "\n",
      "-- Attributes\n",
      "@open:STRING\n",
      "@single_blind:STRING\n",
      "@double_blind:STRING\n",
      "@randomised:STRING\n",
      "@controlled:STRING\n",
      "@placebo:STRING\n",
      "@active_comparator:STRING\n",
      "@crossover:STRING\n",
      "@parallel_group:STRING\n",
      "@arms:STRING\n",
      "\n",
      "-- Overview of attributes\n",
      "open notin {'Yes', 'No'}\n",
      "single_blind notin {'Yes', 'No'}\n",
      "double_blind notin {'Yes', 'No'}\n",
      "randomised notin {'Yes', 'No'}\n",
      "controlled notin {'Yes', 'No'}\n",
      "placebo notin {'Yes', 'No'}\n",
      "active_comparator notin {'Yes', 'No'}\n",
      "crossover notin {'Yes', 'No'}\n",
      "parallel_group notin {'Yes', 'No'}\n",
      "arms notin {'0', '1', '2+'}\n",
      "\n",
      "-- eudract rules for masking\n",
      "open == 'Yes' & single_blind == 'Yes'\n",
      "open == 'Yes' & double_blind == 'Yes'\n",
      "single_blind == 'Yes' & double_blind == 'Yes'\n",
      "open == 'No' & single_blind == 'No' & double_blind == 'No'\n",
      "\n",
      "-- eudract rules for control\n",
      "controlled == 'No' & placebo == 'Yes'\n",
      "controlled == 'No' & active_comparator == 'Yes'\n",
      "\n",
      "-- crossover and parallel cannot occur simultaneously\n",
      "parallel_group == 'Yes' & crossover == 'Yes'\n",
      "\n",
      "-- arms check\n",
      "arms in {'0', '1'} & placebo == 'Yes'\n",
      "arms in {'0', '1'} & active_comparator == 'Yes'\n",
      "   eudract_number arms controlled crossover double_blind open parallel_group  \\\n",
      "0  2005-005218-19   2+        Yes        No           No  Yes             No   \n",
      "1  2005-005218-19   2+        Yes        No           No  Yes             No   \n",
      "\n",
      "  randomised single_blind  \n",
      "0        Yes           No  \n",
      "1        Yes           No  \n",
      "   eudract_number arms controlled crossover double_blind open parallel_group  \\\n",
      "0  2004-000232-91   2+        Yes        No           No  Yes            Yes   \n",
      "1  2004-000232-91   2+        Yes        No           No  Yes            Yes   \n",
      "\n",
      "  randomised single_blind  \n",
      "0        Yes           No  \n",
      "1        Yes           No  \n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "corrupt_dataset = jupyter_helper_functions.load_dataset(\n",
    "    \"../datasets/parker_datasets/eudract/parker-repair/eudract_repair.csv\"\n",
    ")\n",
    "gold_standard = jupyter_helper_functions.load_dataset(\n",
    "    \"../datasets/parker_datasets/eudract/eudract_cleaned_gold_first1000.csv\"\n",
    ")\n",
    "\n",
    "partial_keys = jupyter_helper_functions.load_text_file(\n",
    "    \"../datasets/parker_datasets/eudract/eudract.partialkey\"\n",
    ")\n",
    "rules = jupyter_helper_functions.load_text_file(\n",
    "    \"../datasets/parker_datasets/eudract/eudract.rules\"\n",
    ")\n",
    "\n",
    "# print(partial_keys)\n",
    "# print(rules)\n",
    "# print(corrupt_dataset.head(2))\n",
    "# print(gold_standard.head(2))\n",
    "# print(type(gold_standard.get(\"eudract_number\").iloc[0]))\n",
    "# print(type(corrupt_dataset.get(\"eudract_number\").iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd91d2",
   "metadata": {},
   "source": [
    "# 3. Clean and Merge Data with LLM\n",
    "Use the LLM pipeline to clean and merge the corrupted dataset using the provided rules and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc28a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging groups with LLM:   0%|          | 0/524 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging groups with LLM:  28%|██▊       | 149/524 [04:58<12:31,  2.00s/it]\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash-lite', 'location': 'global'}, 'quotaValue': '30'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '51s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     17\u001b[39m additional_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33mHere are rows of the dataset to provide context for the cleaning process:\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mcorrupt_dataset.sample(rows_of_context).to_string(index=\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     22\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m merged_df = \u001b[43mPipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge_with_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorrupt_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprimary_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprimary_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrpm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrpm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatus_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m time_delta = time.time() - start_time\n\u001b[32m     34\u001b[39m merged_df.to_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m../analysis/repaired_llm/eudract/merged_dataset_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/llm_data_quality_assistant/pipeline.py:100\u001b[39m, in \u001b[36mPipeline.merge_with_llm\u001b[39m\u001b[34m(dataset, primary_key, model_name, rpm, additional_prompt, verbose, status_bar)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmerge_with_llm\u001b[39m(\n\u001b[32m     90\u001b[39m     dataset: pd.DataFrame,\n\u001b[32m   (...)\u001b[39m\u001b[32m     98\u001b[39m     status_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     99\u001b[39m ) -> pd.DataFrame:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge_datasets_by_primary_key\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprimary_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprimary_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrpm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrpm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatus_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatus_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/llm_data_quality_assistant/llm_integration.py:149\u001b[39m, in \u001b[36mmerge_datasets_by_primary_key\u001b[39m\u001b[34m(model_name, primary_key, dataset, verbose, status_bar, rpm, additional_prompt)\u001b[39m\n\u001b[32m    146\u001b[39m row_order = group[\u001b[33m\"\u001b[39m\u001b[33m_tmp_row_order\u001b[39m\u001b[33m\"\u001b[39m].values\n\u001b[32m    147\u001b[39m group.drop(columns=[\u001b[33m\"\u001b[39m\u001b[33m_tmp_row_order\u001b[39m\u001b[33m\"\u001b[39m], inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m merged_row = \u001b[43m__merge_single_corrupted_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m merged_row[\u001b[33m\"\u001b[39m\u001b[33m_tmp_row_order\u001b[39m\u001b[33m\"\u001b[39m] = row_order\n\u001b[32m    156\u001b[39m merged_rows.append(merged_row)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/llm_data_quality_assistant/llm_integration.py:226\u001b[39m, in \u001b[36m__merge_single_corrupted_dataset\u001b[39m\u001b[34m(model_name, dataset, additional_prompt, verbose)\u001b[39m\n\u001b[32m    201\u001b[39m csv = dataset.to_csv(index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    203\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[33mYou are a data cleaning assistant. \u001b[39m\n\u001b[32m    205\u001b[39m \u001b[33mYou will be given a dataset about the same topic, but it may contain errors or inconsistencies. \u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    223\u001b[39m \u001b[33m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m merged_df = \u001b[43m__merge_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m merged_df.shape[\u001b[32m0\u001b[39m] == \u001b[32m0\u001b[39m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    234\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe merged DataFrame is empty. This might be due to the LLM not returning any data. Please check the model and prompt.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    235\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/llm_data_quality_assistant/llm_integration.py:96\u001b[39m, in \u001b[36m__merge_datasets\u001b[39m\u001b[34m(model_name, prompt, dataset, verbose)\u001b[39m\n\u001b[32m     93\u001b[39m         message += chunk\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     message = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mstruct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m data = json.loads(message)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, OpenAIModel):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/llm_data_quality_assistant/llm_models.py:199\u001b[39m, in \u001b[36mGeminiModel.generate\u001b[39m\u001b[34m(self, prompt, format)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    192\u001b[39m     config.update(\n\u001b[32m    193\u001b[39m         {\n\u001b[32m    194\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_mime_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    195\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_schema\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mformat\u001b[39m,\n\u001b[32m    196\u001b[39m         }\n\u001b[32m    197\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    203\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.text \u001b[38;5;28;01mif\u001b[39;00m response.text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/venv/lib/python3.13/site-packages/google/genai/models.py:5630\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5628\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5629\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5630\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5631\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5632\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5633\u001b[39m   logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is done.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   5634\u001b[39m   remaining_remote_calls_afc -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/venv/lib/python3.13/site-packages/google/genai/models.py:4593\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4590\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   4591\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4593\u001b[39m response_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4594\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4595\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n\u001b[32m   4598\u001b[39m   response_dict = _GenerateContentResponse_from_vertex(\n\u001b[32m   4599\u001b[39m       \u001b[38;5;28mself\u001b[39m._api_client, response_dict\n\u001b[32m   4600\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/venv/lib/python3.13/site-packages/google/genai/_api_client.py:755\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    746\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    747\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    750\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    751\u001b[39m ) -> Union[BaseResponse, Any]:\n\u001b[32m    752\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m    753\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m    754\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m755\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    756\u001b[39m   json_response = response.json\n\u001b[32m    757\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/venv/lib/python3.13/site-packages/google/genai/_api_client.py:684\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    677\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m    678\u001b[39m       method=http_request.method,\n\u001b[32m    679\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    682\u001b[39m       timeout=http_request.timeout,\n\u001b[32m    683\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    685\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    686\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    687\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Uni/DataEngineering/bpc_project/venv/lib/python3.13/site-packages/google/genai/errors.py:101\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m     99\u001b[39m status_code = response.status_code\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    103\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash-lite', 'location': 'global'}, 'quotaValue': '30'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '51s'}]}}"
     ]
    }
   ],
   "source": [
    "from llm_data_quality_assistant.pipeline import Pipeline\n",
    "from llm_data_quality_assistant.enums import Models\n",
    "import string\n",
    "import json\n",
    "import jupyter_helper_functions\n",
    "string.punctuation = string.punctuation.replace(\"'\", \"\")  # Remove single quotes from punctuation\n",
    "\n",
    "# Use a primary key for merging\n",
    "primary_key = \"eudract_number\"\n",
    "model = Models.GeminiModels.GEMINI_2_0_FLASH_LITE\n",
    "rows_of_context = 50\n",
    "\n",
    "extra = \"parker_postprocessing_option_2\"\n",
    "file_name = jupyter_helper_functions.sanitize_filename(f\"{model.value}_{rows_of_context}_rows_context_{extra}\")   \n",
    "\n",
    "rpm = 30\n",
    "additional_prompt = f\"\"\"\n",
    "Here are rows of the dataset to provide context for the cleaning process:\n",
    "{corrupt_dataset.sample(rows_of_context).to_string(index=False)}\n",
    "\"\"\"\n",
    "\n",
    "# Merge/clean with LLM\n",
    "merged_df, time_taken = jupyter_helper_functions.merge_with_llm_timed(\n",
    "    dataset = corrupt_dataset,\n",
    "    primary_key = primary_key,\n",
    "    model = model,\n",
    "    rpm = rpm,\n",
    "    additional_prompt = additional_prompt\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f38646d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmerged_df\u001b[49m.to_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m../analysis/repairs/eudract/merged_dataset_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      2\u001b[39m original_corrupted_dataset = pd.read_csv(\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m../datasets/parker_datasets/eudract/eudract_corrupted_first1000.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_data_quality_assistant\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n",
      "\u001b[31mNameError\u001b[39m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Save merged dataset\n",
    "jupyter_helper_functions.save_dataframe_csv(merged_df, f\"../analysis/repairs/eudract/merged_dataset_{file_name}.csv\")\n",
    "\n",
    "original_corrupted_datasets = jupyter_helper_functions.load_dataset(\n",
    "    \"../datasets/parker_datasets/eudract/eudract_corrupted_first1000.csv\"\n",
    ")\n",
    "# Evaluate results\n",
    "jupyter_helper_functions.standardize_and_evaluate(\n",
    "    gold_standard=gold_standard,\n",
    "    merged_df=merged_df,\n",
    "    corrupt_dataset=original_corrupted_datasets,\n",
    "    primary_key=primary_key,\n",
    "    time_delta=time_taken,\n",
    "    results_dir=f\"../analysis/results/eudract/\",\n",
    "    file_name=file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6465270a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
